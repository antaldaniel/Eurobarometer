---
title: "Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  eval = FALSE,
  collapse = TRUE,
  comment = "#>"
)
example_panel_id <- data.frame(
  panel_id = rep(NA_character_, 8),
  id_uniqid = c(101,102,103,104, 108,199,104,188), 
  id_survey = c( rep("eb_23.4", 4), 
                 rep("eb_28.9", 4)),
  filename = c( rep("GESIS_A.sav", 4), 
                rep("GESIS_B.sav", 4)), 
  w1 =  round(runif(8, 0, 1.5),2),
  wex = round(runif(8, 0, 1.5)*runif(8, 1000,9000),2)
  )
example_panel_id$panel_id <- paste0(
  example_panel_id$id_survey, "_", 
  example_panel_id$id_uniqid, "_",
  example_panel_id$filename
)

my_panel <- example_panel_id
```

```{r setup, eval=TRUE, echo=TRUE, message=FALSE}
## This vignette is hypothetical at this stage, and the chunks, 
## unless stated otherwise, are NOT evaluated.
library(eurobarometer)
library(knitr)
library(dplyr)
library(kableExtra)
```

At this point this workflow if purely hypothetical, the functions are not written, and they are not evaluated.

## Acquiring the data from GESIS

I do not see a lot of sense in automating this, because the new GESIS interface requires a lot of approvals and interaction. However, I think that the files should be in a single folder. Maybe this could be a `tempdir()` but I am not very enthusiastic about it because it causes documentation issues.

## Reading in the files

Working with native SPSS files is extremely slow, and I think it would make sense to first read and re-save them as .rds files or .rda files.
```{r, eval=TRUE}
import_file_names <- c(
'ZA7576_sample','ZA7562_sample')

my_survey_list <- read_surveys (
   import_file_names, .f = 'read_example_file' )
```

## Analyze the metadata in the surveys

```{r, eval=TRUE}
my_metadata <- gesis_metadata_create( my_survey_list )
names(my_metadata)
```

The my_metadata table is rather large, so let's review random samples from a few columns.

The variable names need to be harmonized, and you will get suggestions on how to do it. Only a very small subset of the entire table is shown here:

```{r, echo=FALSE, results='asis', eval=TRUE}

my_metadata %>%
  dplyr::filter( var_name_orig %in% c("doi", "isocntry", "qa14_3", "d71b_3", "filename", "d7", "w1", "p1")) %>%
  select ( all_of(c("filename", "var_name_orig", "var_label_orig", "var_name_suggested"))) %>%
  distinct_all() %>%
  distinct ( var_name_suggested, .keep_all =  TRUE ) %>%
  arrange ( var_name_suggested ) %>%
  kable () %>%
  kable_styling(bootstrap_options =
                  c("striped", "hover", "condensed"),
                  fixed_thead = T,
                  font_size = 10 )  

```

The metadata can help identifying questionnaire item types and it suggests conversion to R classes. Again, only a very small subset of the entire table is shown below:

```{r, echo=FALSE, results='asis', eval=TRUE}
my_metadata %>%
  dplyr::filter( var_name_orig %in% c("doi", "isocntry", "qa14_3", "d71b_3", "filename", "d7", "w1", "p1")) %>%
  select ( all_of(
    c("var_label_norm", "var_name_suggested",
      "class_suggested", "n_categories"))) %>%
  distinct_all() %>%
    distinct ( var_name_suggested, .keep_all =  TRUE ) %>%
  arrange ( var_name_suggested ) %>%
  kable () %>%
  kable_styling(bootstrap_options =
                  c("striped", "hover", "condensed"),
                  fixed_thead = T,
                  font_size = 10 )
```

```{r, echo=FALSE, results='asis', eval=TRUE}

my_metadata %>%
  dplyr::filter( var_name_orig %in% c("doi", "isocntry", "qa14_3", "d71b_3", "filename", "d7", "w1", "p1")) %>%
  select ( all_of(
    c("qb", "var_name_orig", "var_name_suggested",
      "class_suggested"))) %>%
  distinct_all() %>%
    distinct ( var_name_suggested, .keep_all =  TRUE ) %>%
  arrange ( qb ) %>%
  kable () %>%
  kable_styling(bootstrap_options =
                  c("striped", "hover", "condensed"),
                  fixed_thead = T,
                  font_size = 10 )  

```

* The `id` groups serve only identification purposes, and will be used in the skeleton of the panel.
* The `metadata` group relate to information about the responses. We include here the country ID because it determines the weight(s) to be used. 
* The `weigths` group shows the weights calculated by Kantar or GESIS.
* The `socio-demography` group shows **identified** socio-demography variables for which `eurobarometer` provides a built-in harmonization tool. There may be other variables that the research would like to add into this group by overrideing the `qb` value of certain question IDs.
* The `trust` group relates to a variable group covered by our example harmonization table `trust_table`.
* The `not_identified` group contains variables for which we do not offer a full-scale built-in harmonization. However, some of our helper functions do help harmonizing these variables, too, but they require more manual programming work by the user.


## Creating a skeleton panel

```{r, eval= TRUE}
## filter out the most basic and omnipresent id variables, and the
## most basic weights, w1 and its projected version wex

my_panel <- panel_create ( survey_list = my_survey_list, 
               ## must be at least two, and one must be the uniqid
               ## of the file or row_id 
                          id_vars = c("uniqid", "doi")
               )

names(my_panel)
```

Let's have a look at 6 randomly selected rows: 

```{r, eval=TRUE, echo=FALSE, results='asis'}
sample_n( my_panel, 6) %>%
  kable() %>%  
  kable_styling(bootstrap_options =
                  c("striped", "hover", "condensed"),
                fixed_thead = T,
                font_size = 10 )
```

This should return a very basic file for joining, a data.frame/tibble with
* a truly unique id
* id elements for joining in individual survey data, in this example, `uniqid` and  `filename` must be present in all imported files. The filename was added by `read_surveys()`.

## Harmonizing various aspects of the survey

```{r, eval=TRUE}

id_panel        <- harmonize_qb_vars( 
                   survey_list = my_survey_list,
                   metadata = my_metadata,
                   id_vars = c("uniqid", "doi"),
                   question_block = "id",
                   var_name = "var_name_suggested",
                   conversion = "class_suggested" ) 

demography_panel <- harmonize_qb_vars( 
                   survey_list = my_survey_list,
                   metadata = my_metadata,
                   id_vars = c("uniqid", "doi"),
                   question_block = "socio-demography",
                   var_name = "var_name_suggested",
                   conversion = "class_suggested" ) 

trust_panel <- harmonize_qb_vars( 
                  survey_list = my_survey_list,
                   metadata = my_metadata,
                   question_block = "trust",
                   var_name = "var_name_suggested",
                   conversion = "class_suggested" ) 
   
weight_panel <- harmonize_qb_vars( survey_list = my_survey_list,
                   metadata = my_metadata,
                   question_block = "weights",
                   id_vars = c("uniqid", "doi"),
                   var_name = "var_name_suggested",
                   conversion = "class_suggested" )

metadata_panel <- harmonize_qb_vars( survey_list = my_survey_list,
                   metadata = my_metadata,
                   question_block = "metadata",
                   var_name = "var_name_suggested",
                   conversion = "class_suggested" ) 
```   

It is likely that your computer's memory will not be enough to left_join these data tables, so bind them in long format:

```{r, eval=TRUE}
panels <- list ( trust_panel,
                 demography_panel, 
                 weight_panel, 
                 metadata_panel)

long_panels <- lapply (panels,
                 function(x) tidyr::pivot_longer (
                   x, -all_of("panel_id") )
                )

panel_long <- do.call(
  rbind, long_panels )

my_panel <- panel_long %>%
  tidyr::pivot_wider () %>%
  left_join ( id_panel, by = 'panel_id'  )
```


The advantage of this workflow is that we can separately work on the 
`demography_panel`, `metadata_panel`, `trust_panel`.

The harmonize_qb in turn cares systematically for variables types, such as _multiple_choice_questions_,  _two-level_factors_, _three-level_factors_, _numeric_, _character_ for constants, etc.

This approach is not sensitive to missing questions.  If some trust questions are present in all files, and others only in a few, you will still get a full panel.

## Summary

```{r, eval=TRUE}
summary(demography_panel[, c(2:ncol(demography_panel))])
```

```{r, eval=TRUE}
summary(trust_panel[, c(2:9)])
```

## Work Documentation

In this approach, all steps are saved and documented, and the basis of the documentation is that the metadata lives separately from the data.

We create a generic function  `document_qb` that orders the variable renaming, value label harmonization, class convsersion steps into a neat table. 

```{r}
## This is how I envision the basic documentation workflow

my_workflow %>%
  left_join ( 
    document_demography ( 
      #documents variables identified by survey rename
      #with the help of demography_table
      working_metadata, 
      working_dir), 
    by = c("id_uniq", "filename")
  ) %>%
  left_join ( 
    document_metadata  ( 
      # documents variables identified by survey rename
      # with the help of survey_metadata_table
      # such as date, people present, etc.
      working_metadata, 
      working_dir), 
    by = c("id_uniq", "filename")
  ) %>%
  left_join ( 
    document_qb ( 
      # documents variables identified by survey rename
      # from a particular question block
      # table part of the package 
      working_metadata, 
      working_dir, 
      document_table = trust_table), 
    by = c("id_uniq", "filename")
  ) %>%
  left_join ( 
    document_qb ( 
      # documents variables identified by survey rename
      # from a particular question block
      # designed by the user 
      working_metadata, 
      working_dir, 
      document_table = user_table_1 ), 
    by = c("id_uniq", "filename")
  ) %>% 
  filter (
    # if only a few survey files had trust variables, then many 
    # trust_vars are missing, chose one for filtering
    !is.na(my_trust_var)
  )
```

Then it creates a standard html or latex table:

```{r}
## this produces as latex or html output with knitr::kable and 
## kableExtra 
## that the user can follow up on

workflow_table ( my_workflow ) %>%
  kableExtra::add_footnote( <users own additions> )
    
```

## Panel Summary

At last we help the user with documenting the panel a bit. 

```{r}
## this produces a readable summary with a package such as 
## stargazer or sjMisc or whatever we chose as a dependency 
## about number of observations, mean, median values, missingness, 
## etc. 

panel_summarize ( my_panel ) 
    
```

## Published results

What I had been corresponding with GESIS is that from there we could have the following outputs:

* When we first finish a big batch, and include it in the panel, we create a trend file that will be published on GESIS, and it will be a separate data publication with its own doi. We can place it, for example, on [Zenodo](https://zenodo.org/record/3759811#.XuYTKkUzbIU) or [figshare](https://figshare.com/account/projects/80837/articles/12389384), too.

* We can also create a methodological publication from the standard steps we made

* We can produce publications of the topical trend elements, probably with other researchers who know more about the topic, such as climate change.

Therefore, we have the possibility to first exploit whatever we create, and the workflow remains transparent and reproducible. Other users can clear up other elements, and if they are good, we can ask them to join their part in as a contribution to the package.

* We can also see how much we are Eurobarometer-specific, and modify the package, or create a mutant for other surveys.  I think we will not be very Eurobarometer specific.
